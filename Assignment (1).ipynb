{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Importing libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Creating a function to get data in HTML format from website.\n",
    "\n",
    "def getHTML(url):\n",
    "    htmlcontent = urlopen(url)\n",
    "    beautify = BeautifulSoup(htmlcontent, 'html.parser')\n",
    "    return beautify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Using a function 'getHTML' to get data from Wikipedia\n",
    "\n",
    "uscities = getHTML('https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Get the whole table as well as links for the each city page.\n",
    "\n",
    "links_table = uscities.find('table', {'class': 'wikitable sortable'})\n",
    "rows = links_table.find_all('tr')\n",
    "\n",
    "for i in range(1, len(rows)):\n",
    "    tds = rows[i].find_all('td')    \n",
    "    if len(tds) != 0:\n",
    "        city_link = tds[1].find('a')\n",
    "        city_info = [td.text.replace('\\n', '').replace('\\xa0', '').replace('\\ufeff', '') for td in tds]\n",
    "        #print(city_link.get('href'))\n",
    "        #print(city_info) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Defining a function to get data from individual city pages. Here we are extacting only name of 'Mayor' for each city.\n",
    "\n",
    "def getAdditionalDetails(url):\n",
    "    try:\n",
    "        city_page = getHTML('https://en.wikipedia.org' + url) \n",
    "        table = city_page.find('table', {'class': 'infobox geography vcard'})\n",
    "        additional_details = []\n",
    "        read_content = False\n",
    "        for tr in table.find_all('tr'):\n",
    "            if (tr.get('class') == ['mergedtoprow'] and not read_content):\n",
    "                link = tr.find('th')\n",
    "                if (link and (link.get_text().strip() == 'Government')):\n",
    "                    read_content = True\n",
    "                    \n",
    "            if ((tr.get('class') == ['mergedrow']) and read_content):               \n",
    "                if (tr.find('th').get_text().strip() == 'â€¢\\xa0Mayor'):\n",
    "                        additional_details.append(tr.find('td').get_text().strip('\\n')) \n",
    "                                   \n",
    "        return additional_details\n",
    "    except Exception as error:\n",
    "        print('Error occured: {}'.format(error))\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Combining all the collected data and defining a dataframe.\n",
    "\n",
    "data_final = []\n",
    "for i in range(1, len(rows)):\n",
    "    tds = rows[i].find_all('td')    \n",
    "    if len(tds) != 0:\n",
    "        city_link = tds[1].find('a')\n",
    "        city_info = [td.text.replace('\\n', '').replace('\\xa0', '').replace('\\ufeff', '') for td in tds]\n",
    "        city_details = getAdditionalDetails(city_link.get('href'))\n",
    "        city_info += city_details\n",
    "        data_final.append(city_info)\n",
    "        \n",
    "df = pd.DataFrame(data_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Define column headings\n",
    "\n",
    "headers = rows[0].find_all('th')\n",
    "headers = [header.get_text().strip('\\n') for header in headers]\n",
    "headers += ['Mayor']\n",
    "headers.insert(7,'2016 Land Area(km2)')\n",
    "headers.insert(9,'2016 Population Density(km2)')\n",
    "#print(headers)\n",
    "df.columns = headers\n",
    "df.rename(columns={'State[c]': 'State'}, inplace = True)\n",
    "df.rename(columns={'2016 land area': '2016 Land area(sqmi)'}, inplace = True)\n",
    "df.rename(columns={'2016 population density': '2016 Population Density(sqmi)'}, inplace = True)\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Data Cleaning\n",
    "\n",
    "for column in df.columns:\n",
    "    df[column] = df[column].str.replace(r\"\\(.*\\)\", \"\")\n",
    "    df[column] = df[column].str.replace(r\"\\[.*\\]\", \"\")\n",
    "    df['Change'] = df['Change'].str.strip('%')\n",
    "    df['2016 Land area(sqmi)'] = df['2016 Land area(sqmi)'].str.strip(\"/sqmi\")\n",
    "    df['2016 Land Area(km2)'] = df['2016 Land Area(km2)'].str.strip(\"/km\")\n",
    "    df['2016 Population Density(sqmi)'] = df['2016 Population Density(sqmi)'].str.strip(\"/sqmi\")\n",
    "    df['2016 Population Density(km2)'] = df['2016 Population Density(km2)'].str.strip(\"/km\")   \n",
    "    #print(df)\n",
    "    \n",
    "df.to_csv(\"Final_Dataset.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
